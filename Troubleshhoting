SIM CARD Tool
Vodafone SIM cards platform
URL: https://vodafone.m-ccp.de/mccp/login
Company ID: 104707
User: admin
PW: vE2xUm%O

Login > Global SIMs > CTRL F then paste sim number > Click on rowTips: View 500 per page, sort by sim number in ASC order.To verify if active:
	1. Data limit should not be exceeded
	2. “Disabled” must be unchecked
“Can connect to internet” must be checked

======================================================================
Gateways GCX & Develco
Examples of two types of GWs and URLs to check SIM status:
 
07000001700F0043 - GCX - https://m2mportal.vodafone.com/m2mscp/
0200000140001123 - Develco - https://vodafone.m-ccp.de/mccp/login
=======================================================================
Install APP
https://prod.backend.gk4null.de/installapp/login
Uname:Installation
Password:India2025
=======================================================================
HawkBit(Prod) - 
https://dm.prod.backend.gk4null.de/UI/login, device-keepers, Hawkbit Password Prod = hBpyE7AthyaBdRnF6nT0CTsNe

=======================================================================
GK DL -dl-dcp-stgk@vodafone.com
=======================================================================
URL to check metrics of a device

gk4null.de/metrics/check/00187612
=======================================================================
Docker
go to 'utils' repo to execute below commands 
 
 1) export DOCKER_HOST="ssh://<username>@prod.backend.gk4null.de"  
2) ./ssh-docker-tunnel -s mongo
 
 To connect to stage -
 
export DOCKER_HOST="ssh://<username>@stage.backend.gk4null.de"
=======================================================================

MongoDB DB list config

mongodb://bundleManager:DB3N8gSzeyepO9CHiCeCAayzB@localhost:4000/?authSource=bundleManager
 
mongodb://deviceBinding:6NJluGlYeIFMefZV0XYGoLrUm@localhost:4000/?authSource=deviceBinding
 
mongodb://gateways:jWhdDPOviR3sNeO9s5RAnYFmO@localhost:4000/?authSource=gateways
 
mongodb://mosquitto:28DZBrCKAsLnSRmjXF3JYVxmw@localhost:4000/?authSource=mosquitto
 
Mosquitto stage
 
mongodb://mosquitto:OAr1cf9sPLKPtCYTDq5Nnuqhs@localhost:4007/?authSource=mosquitto
 
Backend-
 
mongodb://backend:pqZ8HBW13xolqXasdRDYNQT31@localhost:4000/?authSource=backend
=======================================================================
EMQX
./ssh-docker-tunnel -s emqx
 
EMQX dashboard:
Username : device-keepers
EMQX pass: 
Prod: MZ6BdVRH2JSsCIQ1mFswErJNc

Topic:
gateway/02000001400019A3/ssapi/wmbus/request/00130179
 
Payload:
{"type":"get","path":"wm/dev/lansen/repeater/meters","data":{"serialNumber":"00130179"}}
=======================================================================
SGC by Name
https://gk4null.atlassian.net/jira/servicedesk/projects/SGC/boards/199
=======================================================================
Checkes to do-
 
Check pagerduty emails
 
Monteur app: If Monteur app is opening, objects are loading, bind is possible, metrics seen are latest - Kurt-Wein-Straße 2
Admin app: If it is loading.
Install App: If login is possible and gateway installation steps are happening- Installation, India2025; 221024Dort, Ford
Portal: If portal is loading, tree nodes are getting opened till sensors part, Check if heating system tab is getting loaded till table view
Grafana dashboards:
If there are no increasing lags- https://monitoring.gk4null.de/d/rtW0EDenk/kafka-consumer-offsets?orgId=1&refresh=30s&viewPanel=15
CPU usage- Kubernetes / Compute Resources / Pod - Dashboards - Grafana
https://mon.backend.gk4null.de/grafana/d/o8UktW07k/device-keepers-overview?orgId=1&from=now-6h&to=now
prod CPU on page - https://mon.backend.gk4null.de/grafana/d/TLOxhyNWk/cpu-overview?orgId=1
Service wise logs on - https://monitoring.gk4null.de/d/TcgrJnRHz/pod-logs-dashboard?orgId=1&refresh=5s&var-app=redpanda&var-level=All&var-filter_text=&from=now-6h&to=now
 
Instana checks and instana pagerduty alerts
 
Try accessing kubectl port forwarding to postgres db to confirm if kubectl cluster is accessible : kubectl --context=gk4null-prod port-forward service/prod-postgres-timescale-service 30131:5432
 
Try exiting docker cluster and accesing it back- ssh naikm@prod.backend.gk4null.de
 
Check services in docker and lens



If any app - Install/portal/Monteur app is not loading.
 
Check memory, CPU usage, on replica and postgres both dbs on Grafana. 
Check lags if they are normal.
 
Check relevant services for the issue - for eg. docker ps| grep binding 
Check if the containers are running and does not have any error.
 
FOr k8s services check lens or Instana.
 
If you find issue by above steps escalate it it devops with details . If you do not find the issue then too escalate it to devops to check it.
 
Once you escalate it you can check in Instana and in backend if you can still figure out what the issue is.

==========================================================================
GK Instana
https://obs-ccap.instana.io/#/application;appId=HL2EOOK5QmWXWnjTlDxX9Q/services

==========================================================================
GCX Fix

Fixing GCX gateway-
 
Check if GW is in 'unknown' topic/subscription on EMQX dashboard
 
If yes then enanle its VPN (On mqtt shell type: enable-vpn <gw-id>)
To confirm if gw's vpn is enabled ou can run below command in docker shell: cat /mnt/um/data/openvpn/openvpn-status.log
 
Then ssh into the GW from utils repo:  ./gwssh -e prod <gw-id> 
(password is root)
AFter that run run below commands:
 
fw_printenv
fw_setenv production.gateway.eui <gw-id>
fw_setenv product.gateway.eui
reboot

==========================================================================
Postgres login
kubectl --context=gk4null-prod port-forward service/prod-postgres-timescale-service 30131:5432
==========================================================================
Kubernetes

kubectl get svc
 kubectl get nodes
 kubectl get no
2037  kubectl get po
2038  kubectl describe 
2039  kubectl describe sales-portal-reporter-0
2040  kubectl describe po sales-portal-reporter-0
2041  kubectl logs sales-portal-reporter-0
2042  kubectl logs sales-portal-reporter-0 | gerp -i errir
2043  kubectl logs sales-portal-reporter-0 | grep -i error
2044  get get po| grep binding
2045  kubectl get po| grep binding
2046  kubectl get po| grep analytics
2047  kubectl get deployment | grep prod-spot-metric-matcher
2048  kubectl get po -A
2049  kubectl get po -A| grep mirror
 
To restart service in k8s-
 
kubectl get deployment | grep prod-spot-metric-matcher
kubectl rollout restart deployment prod-spot-metric-matcher
 
kubectl get po -A| grep mirror // We do this when the service is not in common namespace
 
---
For checking cron jobs:
 
kubectl get job
kubectl get job prod-superset-cron-jobs-erfurt-current-power-28969382
kubectl get job prod-superset-cron-jobs-erfurt-current-power-28969382 -o yaml
kubectl get cronjob
kubectl edit cronjob prod-superset-cron-jobs-erfurt-em-data
==========================================================================
GW Troubleshooting
Troubleshooting steps -
How to fix devices if they are not sending metrics-
1. Check if GW is ONLINE.
2. Confirm if the device appears in the visibility list of the GW. Command for checking this is: wmbus-list <gw-id>
If yes move to step 3. If not, then check if the GW is in unknown mode. (It is in unknown mode when the metrics are of 7 days or older and wmbus-list is empty). Then to bring into concentrator mode we ssh into the GW and run a command. And reboot the GW.
3. Check if it appears in the bind list by using command >> wmbus-bind-list <gw-id>. If yes move to step 4. If it does not appear in the bind list and it is present in the visibility list then run the wmbus-bind-long command.

Command: wmbus-bind-long <gw-id> <device serial no.> <Device name>
Eg. wmbus-bind-long 02000063468326439705 00124567 LAS RoomSensor 30
4. Next check in Postgres if there are multiple spots created for the device. Delete the old ones and produce tombstone (By hitting the tombstone we direct the device to send metrics to correct spots)
5.  Produce tombstone. We do it to check if devices are not sending empty payloads or in case if a device was reinstalled.
6. Check wmbus-processor container logs:

docker ps | grep wmbus-processor

docker logs <container_id> | grep <flipped device serial>

i) If you see messages as below then you must take a screenshot of your terminal and tell the technician that the device is sending empty payloads. Therefore they must go back and reinstall the external probes or replace them.

{"timestamp":"2024-12-13T14:28:19.240+01:00","message":"Could not extract normalized metrics from decoded wmbus message. There are no kafka events to publish.","level":"INFO","gateway":"0200000140006FE1","deviceAddress":"3330892017001E1B"}

ii) If you see below error then ask if the device is connected correctly, else if it still doesn't work then replace

{"type":"forward","path":"wm/dev/lansen/modbus","error":"Timeout while waiting for modbus master response","status":"error"}

From <https://cps.confluence.agile.vodafone.com/pages/viewpage.action?pageId=431922054&spaceKey=GK&title=Operations%2BGameDay%2B2> 



	Always the same process/options:
	1.Restart the GW. If no change then proceed to step 2.
	2.Connect the GW to an alternate internet source.
	If it is then online:
	Perform FW update if needed.
	If still offline when switching back to SIM, then replace the SIM.
	If still offline on alternate internet source, move GW to place of better reception or replace GW.
	:+1::skin-tone-3:
	
	Additionally, the fact that the only session of data transfer was in Bulgaria means it only worked in assembly process, and has never worked in Germany in actually installation location.

=====================================================================================
Device not sending Metrices
How to fix devices if they are not sending metrics:
 
	1. Check if GW is ONLINE.
	2. Confirm if the device appears in the visibility list of the GW. Command for checking this is : wmbus-list <gw-id>. - If it is not visible we can reboot the GW. If device still doesn’t get visible then we need to ask technician for on-site check.

(for reference- The device (repeater) is not visible to the GW, which is why the devices connected to the repeater are not sending any measured values. Could you please use a sniffer to check the connectivity between device 00133643 and the GW and check the device's batteries? And please repair the device?)

Sometimes even when the device batteries are down then the device goes offline and would not be visible in wmbus-list, so we ask technician to also confirm this is the case.
If device is in listening mode and ON? (Is the device ON, in listening mode mode and batteries are fine, if device is in 3m radis of the GW)
	If it is in 3m radius of the GW. Not too close not too far.
	
	If yes move to step 3. If not then check if the GW is in unknown mode. (It is in unknown mode when wmbus-list is empty ). Then to bring into concentrator mode we ssh into the GW and run a command. And reboot the GW.
	1. Check if it appears in the bind list by using command >> wmbus-bind-list <gw-id>. If yes move to step 4. If it does not appear in the bind list and it is present in the visibility list then run the wmbus-bind-long command.
Eg:
	wmbus-bind-long <gw-id> <device serial no.> <Manufacturer name/abbreviation > <Device type> <Device version>
 
	For eg: wmbus-bind-long 02000063468326439705 00124567 LAS RoomSensor 30
	 
	1. Next check in Postgres if there are multiple spots created for the device. (we check this by using: select * from device_mount dm where dm.meta->>'serial' ='00174432' and dm.removal_date is null)
Delete the old one spot and produce tombstone for new spot id (By hitting the tombstone we direct the device to send metrics to correct spots)

delete by using: delete from device_mount dm where dm.spot_id='00765986-c59f-4319-b506-c0c03ca08eaf'
	 
	Once done, produce tombstone with correct/new spot id. We do it to check if devices are not sending empty payloads or in case if a device was reinstalled.
	 
	1. Check wmbus-processor container logs: 

docker ps| grep wmbus-processor      -- from here we take container id of the service
	docker logs <container_id> | grep <flipped device serial>
	 
	 
	1. If you see messages as below then you must take a screenshot of your terminal and tell the technician that the device is sending empty payloads. Therefore they must go back and reinstall the external probes or replace them.
	 
	{"timestamp":"2024-12-13T14:28:19.240+01:00","message":"Could not extract normalized metrics from decoded wmbus message. There are no kafka events to publish.","level":"INFO","gateway":"0200000140006FE1","deviceAddress":"3330892017001E1B"}
	 
	1. If you see below error in gw-watch: (gw-watch <gw-watch>)
	Ask if the device is connected correctly, else if it still doesn't work then replace-
	 
	{"type":"forward","path":"wm/dev/lansen/modbus","error":"Timeout while waiting for modbus master response","status":"error"}
	 
 
This could again mean either:
1 - modbus converter not there anymore (got moved)
2 - it is not powered anymore for some reason
3 - Too far away or the signal from converter became weak enough that gateway does not receive messages from it anymore
	1. Also sometimes when the device is in bind list but not in visible list then we can suggest to remove the batteries and put it back.
=====================================================================================

Port Forwarding
Portforwarding to below tools:
 
Mongodb:
 
Docker swarm e.g port forward to MOngo
 
./ssh-docker-tunnel -s main_mongodb
 
Influx:
 
kubectl --context=gk4null-prod port-forward svc/prod-influx2-service 8086:8086
 
http://localhost:8086/
 
Redpanda:
 
kubectl --context=gk4null-prod port-forward -n red-panda svc/message-bus-redpanda-console 8080:8080
 
Postgres:
 
kubectl --context=gk4null-prod port-forward service/prod-postgres-timescale-service 30131:5432
